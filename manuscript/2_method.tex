\section{Methodology}
\subsection{DR grading from standard Eye Fundus Images}
For the DR grading, labels are distributed in an heterogeneous manner, as a consequence of the structure underlying the diagnosis of this disease.
As an example, a image annotated as Mild DR has a label closer to the label of a healthy image than to the label of an image showing signs of Proliferative DR.
For this reason, we employ a special technique that accounts for such label structure, which we have introduced in \cite{galdran_cost-sensitive_2020}.
Briefly, we consider a model $U$ producing a prediction $U(x)=\hat{y} \in [0,1]\times \displaystyle \ldots \times[0,1]$, and an associated ground-truth label $y$. 

Our goal is to introduce in the loss computation of a standard CNN trained by backpropgation a cost matrix $M$ that encodes a null cost for a prediction such that $\hat{y}=y_j$, but cost that increases along with the distance $\|y - \hat{y}\|$. 
A simple approach to achieve such increasing label-dependent penalty is by encoding in each row of $M$ those costs, and then computing the scalar product of $\hat{y}$ with the row of $M$ corresponding to $y$, i.e. $\mathcal{L}(y,\hat{y})=\langle M(y,\cdot), \hat{y} \rangle$. 
However, due to the high imbalance of the DR grading problem (with typically few examples of classes DR1, DR3, and DR4) in our experiments we prefer to combine a Cost-Sensitive term with a base loss as follows:
\begin{equation}\label{cs_loss}
\mathcal{L}^{cs}(y,\hat{y}) = \mathcal{L}^{base}(\hat{y},y) + \lambda \langle M^{(2)}(y,\cdot), \hat{y} \rangle, \ \ \  M_{ij}=\|i-j\|_2.
\end{equation}
In the above equation, we have selected the $L^1$-based ground cost matrix $M$, since in our internal validation experiments we noticed this configuration was the one attaining greater quadratic-weighted kappa score, which is the main performance metric in this competition.

In this work, we consider a standard CNN archtitecture, namely ResNet-50 \cite{he_deep_2016}, as it
has established in recent years as a standard in computer vision applications. 
We train this model by using the above regularization and several loss functions as a base error measure, namely Cross-Entropy, Focal Loss \cite{lin_focal_2020}, and Non-Uniform Label Smoothing \cite{galdran_non-uniform_2020}. 
Our models are trained starting from weights pre-trained for the Kaggle Eyepacs competition dataset, based on the above loss function and architecture for each of the two image subsets (OD-centered and macula-centered) independently. 
In addition, we also train a third model on the entire dataset (OD+Macula). 
In test time, for a particular image centered, e.g., in the OD, we combine predictions coming from models trained on OD-centered images with predictions coming from training on both kind of images.

\subsection{Fine-Tuning for UW-image recognition}
The Ultra-Wide Field retinal image dataset on this sub-challenge contains significantly less training examples than the standard fundus image dataset.
For this reason, we fine-tune our best models from the previous section on this new task, again using Cost-Senstive regularization.
In addition, we keep the same strategy as outlined above: we train separate models for OD-centered images, macula-centered  images, and a combination of both.

\subsection{Image Quality Assessment from Retinal Fundus Images}
This sub-challenge encompasses four different sub-tasks, each of them accounting for a different kind of image degradation,and the task is to categorize each degradation into a variable number of classes, ranging from two (Overall Quality) to six (Field Definition). 

In order to potentially improve generalization ability, we train five different models on the available training data. 
First, we train four different image quality assessment models meant to solve each of the four different sub-tasks independently. 
Then we train a last model that solves jointly the four problems. This is done by implementing four different prediction branches towards the end of our architecture, as illustrated in Fig. \ref{fig_architecture}.
In test time, for each sub-task we combine predictions generated from the model trained specifically for that particular task with the model trained in a multi-task manner.
We hypothesize that this approach contributes to the learning of useful representations that can take advantage of certain correlation existing between the different sub-tasks that compose this part of the competition.

\begin{figure*}[h]
\centering
\includegraphics[width=0.9\textwidth]{images/fig1_new.pdf}
\caption{Our multi-task architecture, generating simultaneously predictions for all four sub-tasks in sub-challenge 2.}
\label{fig_architecture}
\end{figure*}
