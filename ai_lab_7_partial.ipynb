{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C-sBZncTFXqF"
   },
   "source": [
    "As in the previous week we start by mounting our drive folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 128
    },
    "colab_type": "code",
    "id": "LM5CoEC5FdmC",
    "outputId": "6da49dea-757a-462a-b197-d61e5dfd7e39"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dOgvSXcyFd6r"
   },
   "source": [
    "And we import tensorflow and keras to our workspace:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rWKsCKfdFYuu"
   },
   "outputs": [],
   "source": [
    "%tensorflow_version 2.x\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SzrJlje0FRmQ"
   },
   "source": [
    "# Convolutional Neural Networks\n",
    "We will now introduce convolutional neural networks, also known as CNNs, a type of deep-learning model almost universally used in computer vision applications. We will see how to apply convnets to image-classification problems.\n",
    "\n",
    "Standard CNNs are basically a stack of **Convolutional** layers (`keras.layers.Conv2D`) followed by **Pooling** layers (`keras.layers.MaxPooling2D`) with interleaved non-linear activation functions. Let us understand this better with some examples:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k_21JRoCFRmS"
   },
   "source": [
    "## Convolutional filters and pooling over batches of images\n",
    "In TensorFlow, each input image is typically represented as a 3D tensor of shape `[height, width, channels]`. A mini-batch is represented as a 4D tensor of shape `[mini-batch size, height, width, channels]`. The weights of a\n",
    "convolutional layer are represented as a 4D tensor of shape `[f_h,f_w,f_m,f_n]`.\n",
    "\n",
    "Let’s look at a simple example. The following code loads two sample images, using `scikit-Learn`'s `load_sample_image()` (which loads two color images, one of a Chinese temple, and the other of a flower) and stacks them into a single `numpy` array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VZT-5PtZFRmT"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_sample_image\n",
    "\n",
    "# Load sample images\n",
    "china = load_sample_image(\"china.jpg\") / 255\n",
    "flower = load_sample_image(\"flower.jpg\") / 255\n",
    "images = np.array([china, flower])\n",
    "batch_size, height, width, channels = images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "u68aUqL_FRme",
    "outputId": "440a2a6b-3fb2-41aa-e9d1-921d64cd8889"
   },
   "outputs": [],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 200
    },
    "colab_type": "code",
    "id": "nG2x6yWdFRmj",
    "outputId": "d79df85e-0365-4d33-d6db-fa2349e4726a"
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(10,10), nrows=1, ncols=2)\n",
    "ax[0].imshow(images[0,:,:,:])\n",
    "ax[0].axis('off')\n",
    "ax[1].imshow(images[1,:,:,:])\n",
    "ax[1].axis('off')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ii2eMyv6FRmr"
   },
   "source": [
    "Let us create two very simple filters and apply them to both images, displaying the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E-m8kwJSFRmt"
   },
   "outputs": [],
   "source": [
    "# Create 2 filters\n",
    "filters = np.zeros(shape=(7, 7, channels, 2), dtype=np.float32)\n",
    "filters[:, 3, :, 0] = 1 # vertical line\n",
    "filters[3, :, :, 1] = 1 # horizontal line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 200
    },
    "colab_type": "code",
    "id": "xMGq6SX1FRmz",
    "outputId": "a50d37dc-0c9d-4553-8e2b-2df9384f7f33"
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(nrows=1, ncols=2)\n",
    "ax[0].imshow(filters[:,:,:,0])\n",
    "ax[1].imshow(filters[:,:,:,1])\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zo1hoD8YFRm8"
   },
   "outputs": [],
   "source": [
    "outputs = tf.nn.conv2d(images, filters, strides=1, padding=\"SAME\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "3kdp2HozGzzY",
    "outputId": "85cad1fa-6ee3-4ed0-9cb9-e2783b89206a"
   },
   "outputs": [],
   "source": [
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 285
    },
    "colab_type": "code",
    "id": "xwRp8kUbFRnB",
    "outputId": "e2f0ad12-1381-4d26-9adc-8424a97d816e"
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(15,15), nrows=1, ncols=2)\n",
    "ax[0].imshow(images[0,:,:])\n",
    "ax[0].axis('off')\n",
    "ax[1].imshow(outputs[0,:,:,0], cmap='gray')\n",
    "ax[1].axis('off')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KU_hFGDgFRnG"
   },
   "source": [
    "In a CNN we use lots of these filters, but we don't want to have to specify them by hand as above. Instead, we want to learn them from our training data. The idea is to keep filtering an image and downsampling the result until we end up with a long one dimensional array of numbers (features) that we can feed to one (or more) Fully-Connected layer that will map that array to a prediction.\n",
    "\n",
    "As an example of this, have a look at this CNN architecture, known as VGG16:\n",
    "\n",
    "![](https://neurohive.io/wp-content/uploads/2018/11/vgg16-1-e1542731207177.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ctx4_dH9FRnJ"
   },
   "source": [
    "In Keras/Tensorflow we perform downsampling with max-pooling layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xDoU6O9GFRnK"
   },
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "wJ7aCWrLGMWg",
    "outputId": "c1cf519c-5ec5-428f-dda6-54964556d322"
   },
   "outputs": [],
   "source": [
    "images.shape, outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 305
    },
    "colab_type": "code",
    "id": "3_fCfO44FRnQ",
    "outputId": "2ec5ea0b-2a73-40b8-faef-b6f0a672289d"
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(15,15), nrows=1, ncols=2)\n",
    "ax[0].imshow(images[0,:,:,0], cmap='gray')\n",
    "ax[1].imshow(outputs[0,:,:,0], cmap='gray')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s_3OCBI6FRnU"
   },
   "source": [
    "## Builindg a CNN in Keras:\n",
    "We will implement a simple CNN to solve the same problem as last week, namely classifying clothes in the Fashion-MNIST dataset. This is quite similar in spirit to the above picture. Let us first build the convolutional part of our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Nmzdcb80FRnV"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-Hyl1Mb0FRnf"
   },
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4bWySbcFFRnj"
   },
   "source": [
    "Note that a CNN takes as input tensors of shape `(batch_size, image_height, image_width, image_channels)`, so we don't need to flatten the input as we did last week. In this case, we configure the CNN to process inputs of size `(28, 28, 1)`, which is the dimensions of Fashion-MNIST images. We do this by passing the argument `input_shape=(28, 28, 1)` to the first layer. Let's display the architecture of the CNN so far:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 345
    },
    "colab_type": "code",
    "id": "_oHAR_PlFRnk",
    "outputId": "db89479c-4eef-44d2-b4d7-5f05ff32e0c8"
   },
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9UgsIr9gFRnq"
   },
   "source": [
    "We see that the output of every `Conv2D` and `MaxPooling2D` layer is a 3D tensor of shape `(height, width,channels)`. The width and height dimensions tend to shrink as we go deeper in the network. The number of channels is controlled by the first argument passed to the `Conv2D` layers (32 or 64).\n",
    "\n",
    "The next step is to feed the last output tensor (of shape (3, 3, 64)) into a densely connected classifier network like those we saw last week: a stack of `Dense` layers. This module will process vectors, which are 1D, whereas the current output is a 3D tensor. Therefore, we first have to flatten the 3D outputs to 1D, and then add a few Dense layers on top.\n",
    "\n",
    "We do 10-category classification, using a final layer with 10 outputs and a softmax activation. Here's what the network looks like now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pLGFsPm6FRns"
   },
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "odtfP3vEFRnx"
   },
   "source": [
    "As you can see, the (3, 3, 64) outputs are flattened into vectors of shape `(576,)` before going through two Dense layers.\n",
    "\n",
    "Let us now train this CNN on our data. The piece of code below is basically the same as last week:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 163
    },
    "colab_type": "code",
    "id": "LwXWkbRlFRny",
    "outputId": "6f454e1e-19af-452f-c007-3bdb0793e4c4"
   },
   "outputs": [],
   "source": [
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZD_3wULrFRn1"
   },
   "outputs": [],
   "source": [
    "X_valid, X_train = X_train_full[:5000] / 255.0, X_train_full[5000:] /255.0\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "pRuaZNwGIc-B",
    "outputId": "36465578-68a2-41aa-b8e2-0a561ba87a79"
   },
   "outputs": [],
   "source": [
    "X_train.shape, X_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qKiqUTcTIc7F"
   },
   "outputs": [],
   "source": [
    "X_train=X_train.reshape((55000, 28, 28,1))\n",
    "X_valid=X_valid.reshape((5000, 28, 28,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VuF1NFsSIEwZ"
   },
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"sgd\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "9U7Y7fznIKRh",
    "outputId": "906860e7-079b-47a8-cda5-e9486f9c7dec"
   },
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SzkGSHhbFRn8"
   },
   "source": [
    "# Transfer Learning\n",
    "You will probably sometimes hear that deep learning only works when lots of data is available. This is valid in part: one fundamental characteristic of deep learning is that it can find interesting features in the training data on its own, without any need for manual feature engineering, and this can only be achieved when lots of training examples are available. This is particularly true for problems where the input samples are very high-dimensional, like images.\n",
    "\n",
    "Fortunately, Deep Learning models are highly repurposable: you can take, say, an image-classification or speech-to-text model trained on a large-scale dataset and reuse it on a significantly different problem with only minor changes. Specifically in the case of computer vision, many pretrained models (usually trained on the Image-\n",
    "Net dataset) are now publicly available for download and allow to train highly accurate vision models out of very little data.\n",
    "\n",
    "We will do so now using dataset intended to perform dogs vs cats classification from a popular Kaggle competition:\n",
    "\n",
    "\n",
    "This dataset contains 25,000 images of dogs and cats (12,500 from each class)I have created a simplified version of this that already  contains three subsets: a training set with 1,000 samples of each class, a validation set with 500 samples of each class, and a test set with 500 samples of each class. The piece of code below should unzip it into your Drive folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "N2xqgRi5FRn_",
    "outputId": "5dcd1d59-d004-408a-9773-04e066cef542"
   },
   "outputs": [],
   "source": [
    "!unzip /content/gdrive/My\\ Drive/LAB7/cats_and_dogs_small.zip -d /content/gdrive/My\\ Drive/LAB7/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mlcv_nH0FRoE"
   },
   "source": [
    "As a sanity check, let’s count how many pictures are in each training split (train/validation/test):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HUV8wf_xFRoF"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "\n",
    "base_dir = '/content/gdrive/My Drive/LAB7/cats_and_dogs_small'\n",
    "\n",
    "train_dir = osp.join(base_dir, 'train')\n",
    "validation_dir = osp.join(base_dir, 'validation')\n",
    "test_dir = osp.join(base_dir, 'test')\n",
    "\n",
    "train_cats_dir = osp.join(train_dir, 'cats')\n",
    "train_dogs_dir = osp.join(train_dir, 'dogs')\n",
    "\n",
    "validation_cats_dir = osp.join(validation_dir, 'cats')\n",
    "validation_dogs_dir = osp.join(validation_dir, 'dogs')\n",
    "\n",
    "test_cats_dir = osp.join(test_dir, 'cats')\n",
    "test_dogs_dir = os.path.join(test_dir, 'dogs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 126
    },
    "colab_type": "code",
    "id": "f4vMe1z_FRoJ",
    "outputId": "8c2d1f61-cf08-4d8a-b9ed-a2ceda012a34"
   },
   "outputs": [],
   "source": [
    "print('total training cat images:', len(os.listdir(train_cats_dir)))\n",
    "print('total training dog images:', len(os.listdir(train_dogs_dir)))\n",
    "print('total validation cat images:', len(os.listdir(validation_cats_dir)))\n",
    "print('total validation dog images:', len(os.listdir(validation_dogs_dir)))\n",
    "print('total test cat images:', len(os.listdir(test_cats_dir)))\n",
    "print('total test dog images:', len(os.listdir(test_dogs_dir)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Oe-lTVhKFRoQ"
   },
   "source": [
    "### Reading data and data augmentation\n",
    "As we know, data should be formatted into appropriately preprocessed floating-point tensors before being fed into the network. Currently, the data is on a drive as JPEG files, so the steps for getting it into the network are roughly as follows:\n",
    "1. Read the picture files.\n",
    "2. Decode the JPEG content to RGB grids of pixels.\n",
    "3. Convert these into floating-point tensors.\n",
    "4. Rescale the pixel values (between 0 and 255) to the [0, 1] interval..\n",
    "Keras has utilities to take care of these steps automatically. Keras has a module with image-processing helper tools, located at keras.preprocessing.image. In particular, it contains the class `ImageDataGenerator`, which lets you quickly set up Python generators that can automatically turn image files on disk into batches of preprocessed tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "Iu8M2w0RFRoR",
    "outputId": "17187d2c-703d-419e-f5dc-107b596d6d81"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uo-3OsB0FRoV"
   },
   "source": [
    "Let’s look at the output of one of these generators: they yield batches of 150x150 RGB images (shape=(20, 150, 150, 3)) and binary labels (shape (20,) ). There are 20 samples in each batch (the batch size)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "V6W2EvFgFRoV",
    "outputId": "dad1f33e-4866-4aae-e358-c0b0c82ae004"
   },
   "outputs": [],
   "source": [
    "for data_batch, labels_batch in train_generator:\n",
    "    print('data batch shape:', data_batch.shape)\n",
    "    print('labels batch shape:', labels_batch.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MHBPWVcuFRoZ"
   },
   "source": [
    "### Data Augmentation\n",
    "Overfitting is caused by having too few samples to learn from, making it very hard to train a model that can generalize to new data. For this reason, it is useful to artificially generate more training data from existing training samples, by augmenting the samples via a number of random transformations.\n",
    "\n",
    "In Keras, this can be done by configuring a number of random transformations to be performed on the images read by the `ImageDataGenerator` instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i0tW_SMkFRob"
   },
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 287
    },
    "colab_type": "code",
    "id": "EGuUgtUAFRoi",
    "outputId": "1c3aabce-f39a-43f2-8f00-d42326f5566c"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import image\n",
    "fnames = [os.path.join(train_cats_dir, fname) for fname in os.listdir(train_cats_dir)]\n",
    "\n",
    "img_path = fnames[3] # choose one image to augment\n",
    "img = image.load_img(img_path, target_size=(150, 150)) # load it and resize to 150x150\n",
    "x = image.img_to_array(img)\n",
    "x = x.reshape((1,) + x.shape) # add a fake batch dimension\n",
    "plt.imshow(image.array_to_img(x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "huMUXtv0FRos",
    "outputId": "7e77352d-c3a2-4cfb-e3cd-bba61d8faaa1"
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "for batch in train_datagen.flow(x, batch_size=1):\n",
    "    plt.figure(i)\n",
    "    imgplot = plt.imshow(image.array_to_img(batch[0]))\n",
    "    i += 1\n",
    "    if i % 4 == 0:\n",
    "        break\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tMEnJemmFRow"
   },
   "source": [
    "If we train a new network using this data-augmentation configuration, the network will never see the same input twice, which will make it harder to overfit to our training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EEKqRB-sFRox"
   },
   "source": [
    "### Using a Pre-Trained CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zU0Usx5yFRox"
   },
   "source": [
    "You pass three arguments to the constructor:\n",
    "* `weights` specifies the weight checkpoint from which to initialize the model.\n",
    "* `include_top` refers to including (or not) the densely connected classifier on top of the network. By default, this densely connected classifier corresponds to the 1,000 classes from ImageNet. Because you intend to use your own densely connected classifier (with only two classes: cat and dog), you don’t need to include it.\n",
    "* `input_shape` is the shape of the image tensors that you’ll feed to the network. This argument is purely optional: if you don’t pass it, the network will be able to process inputs of any size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-DuNKoSJFRoy"
   },
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 854
    },
    "colab_type": "code",
    "id": "AQe1rex-FRo1",
    "outputId": "0e4441ec-98af-4f6c-91c3-a9850f60303a"
   },
   "outputs": [],
   "source": [
    "conv_base.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yIK_BGHcFRo4"
   },
   "source": [
    "The final feature map has shape (4, 4, 512). That’s the feature on top of which we will stick a densely connected classifier. We will now perform fine-tuning on our CNN. This means that we give the following two steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qd9NUEx-FRo5"
   },
   "source": [
    "1. Add our custom network on top of an already-trained base network.\n",
    "2. Jointly train both these layers and the part we added."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vSO3OWJ6FRo5"
   },
   "source": [
    "#### Add a network on top of the pretrained CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 308
    },
    "colab_type": "code",
    "id": "P8LgcHNkFRo6",
    "outputId": "dc3e4d7f-affc-4dca-93a4-c09b8761da8d"
   },
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RVNkfFp2FRo-"
   },
   "source": [
    "#### Train the entire CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "LXSr8HpeFRpE",
    "outputId": "64e72bda-9550-4933-8cc2-94b2a10afbc8"
   },
   "outputs": [],
   "source": [
    "val_datagen = ImageDataGenerator(rescale=1./255) # note, we do not augment validation images\n",
    "validation_generator = test_datagen.flow_from_directory(validation_dir, target_size=(150, 150),\n",
    "                                                        batch_size=20, class_mode='binary')\n",
    "\n",
    "# code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_wNzFVsOFRpH"
   },
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s0jSv4evFRpL"
   },
   "source": [
    "## 1-D CNNs\n",
    "\n",
    "In Keras, you use a 1D convnet via the Conv1D layer, which has an interface similar to Conv2D. It takes as input 3D tensors with shape (samples, time, features) and returns similarly shaped 3D tensors. The convolution window is a 1D window on the temporal axis: axis 1 in the input tensor. \n",
    "\n",
    "Let’s build a simple two-layer 1D convnet and apply it to a sentiment classification task from movie reviews. For that, we work with the IMDB dataset: a set of 50,000 highly polarized reviews from the Internet Movie Database. They’re split into 25,000 reviews for training and 25,000 reviews for testing, each set consisting of 50% negative and 50% positive reviews. The IMDB dataset comes packaged with Keras. It has already been preprocessed: the reviews (sequences of words) have been turned into sequences of integers, where each integer stands for a specific word in a dictionary. The following code will download and prepare the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 163
    },
    "colab_type": "code",
    "id": "ZHQSxNmlFRpN",
    "outputId": "44be5548-fbcb-4748-b776-3c34f4b22485"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "\n",
    "max_features = 10000\n",
    "max_len = 500\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=max_len)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=max_len)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aS6kHAMKFRpS"
   },
   "source": [
    "The argument `max_features=10000` means we only keep the top 10,000 most frequently occurring words in the training data. Rare words will be discarded. This allows to work with vector data of manageable size. We also fix the maximum size of a review to be `max_len=500`.\n",
    "\n",
    "The variables `x_train` and `x_test` are lists of reviews; each review is a list of word indices (encoding a sequence of words). `y_train` and `y_test` are lists of 0s and 1s, where 0 stands for negative and 1 stands for positive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "wwI2nlSPFRpT",
    "outputId": "e9cab821-5cac-47c0-e337-a9ce59eba073"
   },
   "outputs": [],
   "source": [
    "print(y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dIFIdFYzFRpb"
   },
   "source": [
    "1D convnets are structured in the same way as their 2D counterparts, which we used above: they consist of a stack of Conv1D and MaxPooling1D layers, ending in either a global pooling layer or a Flatten layer, that turn the `batch_size x 2D` outputs into `batch_size x 1D` outputs, allowing us to add one or more `Dense` layers to the model for classification or regression.\n",
    "\n",
    "One difference, though, is the fact that we can afford to use larger convolution windows with 1D convnets. With a 2D convolution layer, a `3x3` convolution window contains `3x3 = 9` learnable weights; but with a 1D convolution layer, a convolution window of size 3 contains only 3 weights. We can thus easily afford 1D convolution windows of size 7 or 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RvZKosjBFRpg"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(layers.Embedding(max_features, 128, input_length=max_len))\n",
    "model.add(layers.Conv1D(32, 7, activation='relu'))\n",
    "model.add(layers.MaxPooling1D(5))\n",
    "model.add(layers.Conv1D(32, 7, activation='relu'))\n",
    "model.add(layers.GlobalMaxPooling1D())\n",
    "model.add(layers.Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 381
    },
    "colab_type": "code",
    "id": "nWiVvzYWFRpm",
    "outputId": "61286816-1d0d-4d4a-b646-20ee687e6de4"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WKNnT3HlSWmd"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 381
    },
    "colab_type": "code",
    "id": "FHwbdau9FRpq",
    "outputId": "a982d146-e829-45dd-b888-b3a051edbe3e"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=RMSprop(lr=1e-4), loss='binary_crossentropy', metrics=['acc'])\n",
    "history = model.fit(x_train, y_train, epochs=10,batch_size=128,validation_split=0.2)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "ai_lab_7.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "vessels",
   "language": "python",
   "name": "build_central"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
